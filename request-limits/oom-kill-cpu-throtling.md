# ðŸ”¥ OOM Killer & CPU Throttling in Kubernetes

## 1. Where This Comes From

Kubernetes relies on the **Linux kernel cgroups** to enforce pod resource limits.

* **Memory limits** â†’ enforced strictly â†’ if exceeded â†’ **OOMKill**.
* **CPU limits** â†’ enforced softly â†’ if exceeded â†’ **CPU throttling**.

---

## 2. Memory Requests & Limits â†’ OOM Killer

### Behavior

* **Request (memory)** â†’ used only for scheduling. Node must have that much free RAM.
* **Limit (memory)** â†’ hard maximum.

  * If a container tries to use more â†’ **OOM (Out Of Memory) Killer** terminates it.

### What happens

1. Pod exceeds memory limit.
2. Kernel cgroup detects overuse.
3. Container is killed â†’ status shows `OOMKilled`.
4. Kubelet may restart it (depending on `restartPolicy`).

### Check if OOMKilled

```bash
kubectl describe pod <pod-name>
kubectl get pod <pod-name> -o jsonpath='{.status.containerStatuses[*].lastState.terminated.reason}'
```

### Example

```yaml
resources:
  requests:
    memory: "256Mi"
  limits:
    memory: "512Mi"
```

ðŸ”¹ Pod is scheduled if node has â‰¥ 256Mi RAM.
ðŸ”¹ If it tries to use >512Mi â†’ **OOMKilled**.

---

## 3. CPU Requests & Limits â†’ Throttling

### Behavior

* **Request (CPU)** â†’ Scheduler guarantees CPU cycles.
* **Limit (CPU)** â†’ Upper bound.

  * Exceeding limit does **not kill** the pod.
  * Instead, Linux cgroups **throttle CPU** usage.

### What happens

1. Pod requests `500m` (0.5 core), limit is `1` (1 core).
2. Pod can burst up to 1 core.
3. If it tries to use >1 core â†’ kernel throttles â†’ pod slows down, no kill.

### Example

```yaml
resources:
  requests:
    cpu: "500m"
  limits:
    cpu: "1"
```

ðŸ”¹ Pod is guaranteed half a CPU.
ðŸ”¹ Can burst up to 1 CPU.
ðŸ”¹ If app tries to use 2 CPUs â†’ throttled back to 1.

---

## 4. Combined Example â€“ OOMKill + CPU Throttling

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: demo
spec:
  containers:
  - name: stress
    image: polinux/stress
    args: ["--cpu", "2", "--vm", "1", "--vm-bytes", "700M", "--vm-hang", "1"]
    resources:
      requests:
        memory: "256Mi"
        cpu: "500m"
      limits:
        memory: "512Mi"
        cpu: "1"
```

ðŸ”¹ What happens:

* Pod scheduled only if node has â‰¥0.5 CPU and â‰¥256Mi RAM.
* If it uses >1 CPU â†’ throttled.
* If it uses >512Mi â†’ killed by OOM killer.

---

## 5. QoS Class Interaction

* **Guaranteed (requests == limits)** â†’ least likely to be OOMKilled if node runs out of memory.
* **Burstable** â†’ more likely to be killed than Guaranteed.
* **BestEffort** â†’ killed first (no requests/limits set).

---

## 6. Debugging

Check pod events:

```bash
kubectl describe pod <pod>
```

Check container termination reason:

```bash
kubectl get pod <pod> -o jsonpath='{.status.containerStatuses[0].lastState.terminated.reason}'
```

Check resource usage live:

```bash
kubectl top pod
kubectl top node
```

---

## 7. Best Practices

âœ… Always set both **requests** and **limits** â†’ prevents cluster starvation.
âœ… For latency-sensitive apps â†’ set **requests == limits** â†’ no throttling surprises.
âœ… Monitor OOMKills with Prometheus/Grafana â†’ catch memory leaks.
âœ… Use **Vertical Pod Autoscaler (VPA)** to adjust requests automatically.
âœ… Use **HPA (Horizontal Pod Autoscaler)** for CPU-based scaling.
