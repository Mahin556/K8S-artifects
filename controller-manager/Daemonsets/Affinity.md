Perfect example üëç ‚Äî you‚Äôve nailed the difference between **nodeSelector** and **nodeAffinity** in DaemonSets. Let me expand on it with some detailed notes so it‚Äôs crystal clear:

* **nodeSelector vs nodeAffinity**

  * `nodeSelector`: Simple equality-based matching (`key=value`). Very limited.
  * `nodeAffinity`: More expressive, supports operators (`In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt`, `Lt`) and **two modes**:

    * `requiredDuringSchedulingIgnoredDuringExecution`: **Hard rule** ‚Äì if no node matches, the Pod won‚Äôt be scheduled.
    * `preferredDuringSchedulingIgnoredDuringExecution`: **Soft rule** ‚Äì scheduler tries to place Pods there, but falls back if unavailable.

* **DaemonSet + Node Affinity**
  DaemonSets schedule Pods on *all eligible nodes*. Using affinity lets you filter which nodes are eligible:

  * **Required rule** ensures only nodes labeled `type=platform-tools` will ever get a Pod.
  * **Preferred rule** makes the scheduler *try* to pick nodes with `instance-type=t2.large` if available.

* **Your YAML** breakdown:

  ```yaml
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: type
            operator: In
            values:
            - platform-tools
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: instance-type
            operator: In
            values:
            - t2.large
  ```

  * Pods **must** be scheduled only on nodes with `type=platform-tools`.
  * If multiple nodes have that label, scheduler **prefers** nodes labeled `instance-type=t2.large`.
  * If no such preferred node exists, Pods will still run on other `platform-tools` nodes.

* **Use cases**

  * Run logging agents only on worker nodes (`required`) and prefer high-memory nodes (`preferred`).
  * Run GPU monitoring DaemonSets only on GPU-enabled nodes.
  * Run storage agents only on SSD-backed nodes.

* **Pro tip:**
  You can combine **affinity** + **tolerations** in DaemonSets:

  * Taint nodes you want to isolate.
  * Add tolerations in the DaemonSet to allow only specific agents (like logging, monitoring) to run there.

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
  labels:
    app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd
  template:
    metadata:
      labels:
        name: fluentd
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: type
                operator: In
                values:
                - platform-tools
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
              - key: instance-type
                operator: In
                values:
                - t2.large
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
```

### References:
- https://devopscube.com/kubernetes-daemonset/